{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=0, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import PointStruct\n",
    "\n",
    "URL = os.getenv(\"QDRANT_URL\")\n",
    "API_KEY = os.getenv(\"QDRANT_API_KEY\")\n",
    "COLLECTION_NAME = os.getenv(\"QDRANT_COLLECTION_NAME\")\n",
    "\n",
    "client = QdrantClient(url=URL, api_key=API_KEY)\n",
    "\n",
    "# Add multiple keys in payload\n",
    "points = [\n",
    "    PointStruct(\n",
    "        id=1,\n",
    "        vector=[0.1, 0.2, 0.3],  # Example vector\n",
    "        payload={\n",
    "            \"category\": \"news\",\n",
    "            \"timestamp\": \"2024-12-30T10:00:00Z\",\n",
    "            \"author\": \"Alice\",\n",
    "        },\n",
    "    ),\n",
    "    PointStruct(\n",
    "        id=2,\n",
    "        vector=[0.4, 0.5, 0.6],  # Example vector\n",
    "        payload={\n",
    "            \"category\": \"blog\",\n",
    "            \"timestamp\": \"2024-12-29T15:30:00Z\",\n",
    "            \"author\": \"Bob\",\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Upsert points into Qdrant\n",
    "client.upsert(collection_name=\"test\", points=points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.models import Filter, FieldCondition, MatchValue\n",
    "\n",
    "# Create a filter to search by multiple keys\n",
    "filter_query = Filter(\n",
    "    must=[\n",
    "        FieldCondition(key=\"category\", match=MatchValue(value=\"news\")),\n",
    "        FieldCondition(key=\"author\", match=MatchValue(value=\"Alice\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Search with the filter\n",
    "result = client.search(\n",
    "    collection_name=\"test\",\n",
    "    query_vector=[0.1, 0.2, 0.3],  # Query vector\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast language models are crucial in the field of natural language processing (NLP) due to their ability to quickly and efficiently process and generate human-like language. The importance of fast language models can be seen in several areas:\n",
      "\n",
      "1. **Real-time Applications**: Fast language models enable real-time applications such as chatbots, virtual assistants, and language translation software. These models can quickly respond to user input, making the interaction more seamless and natural.\n",
      "2. **Large-Scale Deployment**: Fast language models can be deployed on large-scale platforms, handling a high volume of requests and user interactions. This is particularly important for applications such as customer service chatbots, where speed and efficiency are critical.\n",
      "3. **Improved User Experience**: Fast language models can significantly improve the user experience by reducing latency and providing quicker responses. This leads to increased user engagement, satisfaction, and loyalty.\n",
      "4. **Efficient Data Processing**: Fast language models can process large amounts of data quickly, making them ideal for applications that involve analyzing and generating text, such as text summarization, sentiment analysis, and language translation.\n",
      "5. **Competitive Advantage**: Companies that adopt fast language models can gain a competitive advantage over their peers. By providing faster and more efficient language processing capabilities, they can differentiate themselves and attract more customers.\n",
      "6. **Cost Savings**: Fast language models can also lead to cost savings by reducing the computational resources required for language processing tasks. This can result in lower infrastructure costs and improved scalability.\n",
      "7. **Enabling New Applications**: Fast language models can enable new applications and use cases, such as:\n",
      "\t* Real-time language translation for multilingual conversations.\n",
      "\t* Quick generation of content, such as articles, social media posts, and product descriptions.\n",
      "\t* Intelligent writing assistants that provide instant feedback and suggestions.\n",
      "8. **Enhanced Security**: Fast language models can also be used to enhance security by quickly detecting and responding to potential threats, such as phishing attacks, spam, and malware.\n",
      "9. **Improved Accessibility**: Fast language models can improve accessibility for people with disabilities, such as those who are visually impaired or have speech disorders. By providing quick and accurate language processing, these models can help bridge the communication gap.\n",
      "10. **Advancements in Research**: Fast language models can accelerate research in NLP and related fields, such as artificial intelligence, machine learning, and cognitive science. By providing a foundation for further research and development, these models can help drive innovation and advancements in the field.\n",
      "\n",
      "Some examples of fast language models include:\n",
      "\n",
      "* Transformer-based models, such as BERT, RoBERTa, and XLNet.\n",
      "* Recurrent neural network (RNN) models, such as long short-term memory (LSTM) and gated recurrent units (GRU).\n",
      "* Attention-based models, such as attention-based RNNs and Transformer-XL.\n",
      "\n",
      "These models have achieved state-of-the-art results in various NLP tasks, including language translation, question answering, text classification, and sentiment analysis.\n"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "import os\n",
    "\n",
    "client = Groq(\n",
    "    api_key=\"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\",\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the importance of fast language models\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    stream=False,\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
